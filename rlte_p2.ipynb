{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u6KKIQHDY4c",
        "outputId": "801a34f3-b258-4374-82a5-5ee7481619ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- [Step 1] Setting up Environment ---\n",
            "1. Cloning repository 'rtle_parallelized'...\n",
            "Cloning into 'rtle_parallelized'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 46 (delta 3), reused 46 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (46/46), 8.37 MiB | 14.90 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "2. Installing dependencies...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h3. Checking Hardware...\n",
            "   SUCCESS: GPU Detected: Tesla T4\n",
            "\n",
            "Setup complete! Proceed to Step 2.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# Step 1: Project Setup & Initialization\n",
        "# ==========================================\n",
        "# This step prepares the Colab environment for the repo-based simulation.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "REPO_URL = \"https://github.com/SalmanSattar24/rtle_parallelized\"\n",
        "REPO_DIR = \"rtle_parallelized\"\n",
        "\n",
        "print(\"--- [Step 1] Setting up Environment ---\")\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    print(f\"1. Cloning repository '{REPO_DIR}'...\")\n",
        "    !git clone {REPO_URL}\n",
        "else:\n",
        "    print(f\"1. Repository '{REPO_DIR}' already exists. Skipping clone.\")\n",
        "\n",
        "print(\"2. Installing dependencies...\")\n",
        "!pip install -q -r {REPO_DIR}/requirements.txt\n",
        "!pip install -q gymnasium\n",
        "\n",
        "if REPO_DIR not in sys.path:\n",
        "    sys.path.append(REPO_DIR)\n",
        "\n",
        "print(\"3. Checking Hardware...\")\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"   SUCCESS: GPU Detected: {device_name}\")\n",
        "else:\n",
        "    print(\"   WARNING: No GPU detected. Enable GPU in Runtime > Change runtime type.\")\n",
        "\n",
        "print(\"\\nSetup complete! Proceed to Step 2.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27220987",
        "outputId": "cf53495b-1074-4aa5-d383-9319fdebcf1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Config:\n",
            "  env_type=strategic, num_lots=40, terminal_time=150, time_delta=15\n",
            "  num_envs=8, num_steps=10, total_timesteps=16000\n",
            "  n_eval_episodes=20, drop_feature=None\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# Step 2: Configure Training Settings\n",
        "# ==========================================\n",
        "# This keeps the repo logic unchanged while tuning runtime.\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Paper-aligned defaults (adjust for runtime)\n",
        "env_type = \"strategic\"\n",
        "num_lots = 40\n",
        "terminal_time = 150\n",
        "time_delta = 15\n",
        "num_envs = 8\n",
        "num_steps = 10\n",
        "total_timesteps = num_envs * num_steps * 200\n",
        "n_eval_episodes = 20\n",
        "drop_feature = None\n",
        "\n",
        "os.makedirs(f\"{REPO_DIR}/rewards\", exist_ok=True)\n",
        "os.makedirs(f\"{REPO_DIR}/models\", exist_ok=True)\n",
        "os.makedirs(f\"{REPO_DIR}/tensorboard_logs\", exist_ok=True)\n",
        "\n",
        "print(\"Config:\")\n",
        "print(f\"  env_type={env_type}, num_lots={num_lots}, terminal_time={terminal_time}, time_delta={time_delta}\")\n",
        "print(f\"  num_envs={num_envs}, num_steps={num_steps}, total_timesteps={total_timesteps}\")\n",
        "print(f\"  n_eval_episodes={n_eval_episodes}, drop_feature={drop_feature}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f8ea393",
        "outputId": "a68887e5-ab61-4471-992e-e11c55ca49ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation shape: (8, 107)\n",
            "Action space: Box(-10.0, 10.0, (7,), float32)\n",
            "Environment ready. Proceed to Step 4.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# Step 3: Repo Environment Sanity Check\n",
        "# ==========================================\n",
        "# This uses the real Market environment from the repo.\n",
        "\n",
        "import gymnasium as gym\n",
        "from simulation.market_gym import Market\n",
        "\n",
        "def make_env(config):\n",
        "    def thunk():\n",
        "        return Market(config)\n",
        "    return thunk\n",
        "\n",
        "configs = [\n",
        "    {\n",
        "        \"market_env\": env_type,\n",
        "        \"execution_agent\": \"rl_agent\",\n",
        "        \"volume\": num_lots,\n",
        "        \"seed\": seed + i,\n",
        "        \"terminal_time\": terminal_time,\n",
        "        \"time_delta\": time_delta,\n",
        "        \"drop_feature\": drop_feature,\n",
        "    }\n",
        "    for i in range(num_envs)\n",
        " ]\n",
        "env_fns = [make_env(cfg) for cfg in configs]\n",
        "envs = gym.vector.AsyncVectorEnv(env_fns=env_fns)\n",
        "obs, info = envs.reset(seed=seed)\n",
        "print(f\"Observation shape: {obs.shape}\")\n",
        "print(f\"Action space: {envs.single_action_space}\")\n",
        "print(\"Environment ready. Proceed to Step 4.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aca114c2",
        "outputId": "8ee63908-8ccb-4b14-fbc0-d6e307b13f35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training via repo's actor_critic.py...\n",
            "2026-02-10 00:10:12.941148: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770682212.960987     469 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770682212.966989     469 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770682212.983056     469 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770682212.983081     469 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770682212.983085     469 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770682212.983090     469 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-10 00:10:12.987538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "starting the training process\n",
            "environment set up: volume=40, market_env=strategic\n",
            "\n",
            "-----\n",
            "batch_size=80, minibatch_size=80, num_iterations=200, learning_rate=0.0005, num_iterations=200, num_envs=8, num_steps_per_env=10, n_evalutation_episodes=20\n",
            "-----\n",
            "\n",
            "\n",
            "time_delta=15, terminal_time=150, lots=40\n",
            "-----\n",
            "no additional tag for the experiment\n",
            "the run name is: strategic_40_seed_0_eval_seed_100_eval_episodes_20_num_iterations_200_bsize_80_log_normal\n",
            "writing summary to /content/rtle_parallelized/tensorboard_logs/strategic_40_seed_0_eval_seed_100_eval_episodes_20_num_iterations_200_bsize_80_log_normal:\n",
            "Number of GPUs available: 1\n",
            "Using GPU: Tesla T4 (cuda:0)\n",
            "not dropping any feature\n",
            "observation space: Box(-inf, inf, (107,), float32), action space: Box(-10.0, 10.0, (7,), float32)\n",
            "the agent type is: log_normal\n",
            "iteration=0\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "SPS: 19\n",
            "iteration=1\n",
            "SPS: 23\n",
            "iteration=2\n",
            "SPS: 21\n",
            "iteration=3\n",
            "SPS: 23\n",
            "iteration=4\n",
            "SPS: 24\n",
            "iteration=5\n",
            "SPS: 24\n",
            "iteration=6\n",
            "SPS: 24\n",
            "iteration=7\n",
            "SPS: 24\n",
            "iteration=8\n",
            "SPS: 24\n",
            "iteration=9\n",
            "SPS: 25\n",
            "iteration=10\n",
            "SPS: 25\n",
            "iteration=11\n",
            "SPS: 24\n",
            "iteration=12\n",
            "SPS: 24\n",
            "iteration=13\n",
            "SPS: 25\n",
            "iteration=14\n",
            "SPS: 25\n",
            "iteration=15\n",
            "SPS: 24\n",
            "iteration=16\n",
            "SPS: 25\n",
            "iteration=17\n",
            "SPS: 25\n",
            "iteration=18\n",
            "SPS: 25\n",
            "iteration=19\n",
            "SPS: 24\n",
            "iteration=20\n",
            "SPS: 25\n",
            "iteration=21\n",
            "SPS: 25\n",
            "iteration=22\n",
            "SPS: 25\n",
            "iteration=23\n",
            "SPS: 25\n",
            "iteration=24\n",
            "SPS: 25\n",
            "iteration=25\n",
            "SPS: 25\n",
            "iteration=26\n",
            "SPS: 25\n",
            "iteration=27\n",
            "SPS: 25\n",
            "iteration=28\n",
            "SPS: 25\n",
            "iteration=29\n",
            "SPS: 25\n",
            "iteration=30\n",
            "SPS: 25\n",
            "iteration=31\n",
            "SPS: 25\n",
            "iteration=32\n",
            "SPS: 25\n",
            "iteration=33\n",
            "SPS: 25\n",
            "iteration=34\n",
            "SPS: 25\n",
            "iteration=35\n",
            "SPS: 25\n",
            "iteration=36\n",
            "SPS: 25\n",
            "iteration=37\n",
            "SPS: 25\n",
            "iteration=38\n",
            "SPS: 25\n",
            "iteration=39\n",
            "SPS: 25\n",
            "iteration=40\n",
            "SPS: 25\n",
            "iteration=41\n",
            "SPS: 25\n",
            "iteration=42\n",
            "SPS: 25\n",
            "iteration=43\n",
            "SPS: 25\n",
            "iteration=44\n",
            "SPS: 25\n",
            "iteration=45\n",
            "SPS: 25\n",
            "iteration=46\n",
            "SPS: 25\n",
            "iteration=47\n",
            "SPS: 25\n",
            "iteration=48\n",
            "SPS: 25\n",
            "iteration=49\n",
            "SPS: 25\n",
            "iteration=50\n",
            "SPS: 25\n",
            "iteration=51\n",
            "SPS: 25\n",
            "iteration=52\n",
            "SPS: 25\n",
            "iteration=53\n",
            "SPS: 25\n",
            "iteration=54\n",
            "SPS: 25\n",
            "iteration=55\n",
            "SPS: 25\n",
            "iteration=56\n",
            "SPS: 25\n",
            "iteration=57\n",
            "SPS: 25\n",
            "iteration=58\n",
            "SPS: 25\n",
            "iteration=59\n",
            "SPS: 25\n",
            "iteration=60\n",
            "SPS: 25\n",
            "iteration=61\n",
            "SPS: 25\n",
            "iteration=62\n",
            "SPS: 25\n",
            "iteration=63\n",
            "SPS: 25\n",
            "iteration=64\n",
            "SPS: 25\n",
            "iteration=65\n",
            "SPS: 25\n",
            "iteration=66\n",
            "SPS: 25\n",
            "iteration=67\n",
            "SPS: 25\n",
            "iteration=68\n",
            "SPS: 25\n",
            "iteration=69\n",
            "SPS: 25\n",
            "iteration=70\n",
            "SPS: 25\n",
            "iteration=71\n",
            "SPS: 25\n",
            "iteration=72\n",
            "SPS: 25\n",
            "iteration=73\n",
            "SPS: 25\n",
            "iteration=74\n",
            "SPS: 25\n",
            "iteration=75\n",
            "SPS: 25\n",
            "iteration=76\n",
            "SPS: 25\n",
            "iteration=77\n",
            "SPS: 25\n",
            "iteration=78\n",
            "SPS: 25\n",
            "iteration=79\n",
            "SPS: 25\n",
            "iteration=80\n",
            "SPS: 25\n",
            "iteration=81\n",
            "SPS: 25\n",
            "iteration=82\n",
            "SPS: 25\n",
            "iteration=83\n",
            "SPS: 25\n",
            "iteration=84\n",
            "SPS: 25\n",
            "iteration=85\n",
            "SPS: 25\n",
            "iteration=86\n",
            "SPS: 25\n",
            "iteration=87\n",
            "SPS: 25\n",
            "iteration=88\n",
            "SPS: 25\n",
            "iteration=89\n",
            "SPS: 25\n",
            "iteration=90\n",
            "SPS: 25\n",
            "iteration=91\n",
            "SPS: 25\n",
            "iteration=92\n",
            "SPS: 25\n",
            "iteration=93\n",
            "SPS: 25\n",
            "iteration=94\n",
            "SPS: 25\n",
            "iteration=95\n",
            "SPS: 25\n",
            "iteration=96\n",
            "SPS: 25\n",
            "iteration=97\n",
            "SPS: 25\n",
            "iteration=98\n",
            "SPS: 25\n",
            "iteration=99\n",
            "SPS: 25\n",
            "iteration=100\n",
            "SPS: 25\n",
            "iteration=101\n",
            "SPS: 25\n",
            "iteration=102\n",
            "SPS: 25\n",
            "iteration=103\n",
            "SPS: 25\n",
            "iteration=104\n",
            "SPS: 25\n",
            "iteration=105\n",
            "SPS: 25\n",
            "iteration=106\n",
            "SPS: 25\n",
            "iteration=107\n",
            "SPS: 25\n",
            "iteration=108\n",
            "SPS: 25\n",
            "iteration=109\n",
            "SPS: 25\n",
            "iteration=110\n",
            "SPS: 25\n",
            "iteration=111\n",
            "SPS: 25\n",
            "iteration=112\n",
            "SPS: 25\n",
            "iteration=113\n",
            "SPS: 25\n",
            "iteration=114\n",
            "SPS: 25\n",
            "iteration=115\n",
            "SPS: 25\n",
            "iteration=116\n",
            "SPS: 25\n",
            "iteration=117\n",
            "SPS: 25\n",
            "iteration=118\n",
            "SPS: 25\n",
            "iteration=119\n",
            "SPS: 25\n",
            "iteration=120\n",
            "SPS: 25\n",
            "iteration=121\n",
            "SPS: 25\n",
            "iteration=122\n",
            "SPS: 25\n",
            "iteration=123\n",
            "SPS: 25\n",
            "iteration=124\n",
            "SPS: 25\n",
            "iteration=125\n",
            "SPS: 25\n",
            "iteration=126\n",
            "SPS: 25\n",
            "iteration=127\n",
            "SPS: 25\n",
            "iteration=128\n",
            "SPS: 25\n",
            "iteration=129\n",
            "SPS: 25\n",
            "iteration=130\n",
            "SPS: 25\n",
            "iteration=131\n",
            "SPS: 25\n",
            "iteration=132\n",
            "SPS: 25\n",
            "iteration=133\n",
            "SPS: 25\n",
            "iteration=134\n",
            "SPS: 25\n",
            "iteration=135\n",
            "SPS: 25\n",
            "iteration=136\n",
            "SPS: 25\n",
            "iteration=137\n",
            "SPS: 25\n",
            "iteration=138\n",
            "SPS: 25\n",
            "iteration=139\n",
            "SPS: 25\n",
            "iteration=140\n",
            "SPS: 25\n",
            "iteration=141\n",
            "SPS: 25\n",
            "iteration=142\n",
            "SPS: 25\n",
            "iteration=143\n",
            "SPS: 25\n",
            "iteration=144\n",
            "SPS: 25\n",
            "iteration=145\n",
            "SPS: 25\n",
            "iteration=146\n",
            "SPS: 25\n",
            "iteration=147\n",
            "SPS: 25\n",
            "iteration=148\n",
            "SPS: 25\n",
            "iteration=149\n",
            "SPS: 25\n",
            "iteration=150\n",
            "SPS: 25\n",
            "iteration=151\n",
            "SPS: 25\n",
            "iteration=152\n",
            "SPS: 25\n",
            "iteration=153\n",
            "SPS: 25\n",
            "iteration=154\n",
            "SPS: 25\n",
            "iteration=155\n",
            "SPS: 25\n",
            "iteration=156\n",
            "SPS: 25\n",
            "iteration=157\n",
            "SPS: 25\n",
            "iteration=158\n",
            "SPS: 25\n",
            "iteration=159\n",
            "SPS: 25\n",
            "iteration=160\n",
            "SPS: 25\n",
            "iteration=161\n",
            "SPS: 25\n",
            "iteration=162\n",
            "SPS: 25\n",
            "iteration=163\n",
            "SPS: 25\n",
            "iteration=164\n",
            "SPS: 25\n",
            "iteration=165\n",
            "SPS: 25\n",
            "iteration=166\n",
            "SPS: 25\n",
            "iteration=167\n",
            "SPS: 25\n",
            "iteration=168\n",
            "SPS: 25\n",
            "iteration=169\n",
            "SPS: 25\n",
            "iteration=170\n",
            "SPS: 25\n",
            "iteration=171\n",
            "SPS: 25\n",
            "iteration=172\n",
            "SPS: 25\n",
            "iteration=173\n",
            "SPS: 25\n",
            "iteration=174\n",
            "SPS: 25\n",
            "iteration=175\n",
            "SPS: 25\n",
            "iteration=176\n",
            "SPS: 25\n",
            "iteration=177\n",
            "SPS: 25\n",
            "iteration=178\n",
            "SPS: 25\n",
            "iteration=179\n",
            "SPS: 25\n",
            "iteration=180\n",
            "SPS: 25\n",
            "iteration=181\n",
            "SPS: 25\n",
            "iteration=182\n",
            "SPS: 25\n",
            "iteration=183\n",
            "SPS: 25\n",
            "iteration=184\n",
            "SPS: 25\n",
            "iteration=185\n",
            "SPS: 25\n",
            "iteration=186\n",
            "SPS: 25\n",
            "iteration=187\n",
            "SPS: 25\n",
            "iteration=188\n",
            "SPS: 25\n",
            "iteration=189\n",
            "SPS: 25\n",
            "iteration=190\n",
            "SPS: 25\n",
            "iteration=191\n",
            "SPS: 25\n",
            "iteration=192\n",
            "SPS: 25\n",
            "iteration=193\n",
            "SPS: 25\n",
            "iteration=194\n",
            "SPS: 25\n",
            "iteration=195\n",
            "SPS: 25\n",
            "iteration=196\n",
            "SPS: 25\n",
            "iteration=197\n",
            "SPS: 25\n",
            "iteration=198\n",
            "SPS: 25\n",
            "iteration=199\n",
            "SPS: 25\n",
            "model saved to /content/rtle_parallelized/models/strategic_40_seed_0_eval_seed_100_eval_episodes_20_num_iterations_200_bsize_80_log_normal.pt\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# Step 4: Run Repo Training (Logistic-Normal Policy)\n",
        "# ==========================================\n",
        "# Training only; evaluation is handled in the Quick Eval cell.\n",
        "\n",
        "drop_feature_cli = \"None\" if drop_feature is None else drop_feature\n",
        "print(\"Starting training via repo's actor_critic.py...\")\n",
        "!python {REPO_DIR}/rl_files/actor_critic.py \\\n",
        "  --exp_name log_normal \\\n",
        "  --env_type {env_type} \\\n",
        "  --num_lots {num_lots} \\\n",
        "  --terminal_time {terminal_time} \\\n",
        "  --time_delta {time_delta} \\\n",
        "  --num_envs {num_envs} \\\n",
        "  --num_steps {num_steps} \\\n",
        "  --total_timesteps {total_timesteps} \\\n",
        "  --n_eval_episodes {n_eval_episodes} \\\n",
        "  --drop_feature {drop_feature_cli} \\\n",
        "  --cuda \\\n",
        "  --no-evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: rtle_parallelized/models/strategic_40_seed_0_eval_seed_100_eval_episodes_20_num_iterations_200_bsize_80_log_normal.pt\n",
            "Quick eval mean reward: -1.4200, std: 2.3436, n=20\n",
            "Remaining volume at cutoff (mean): 0.00\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# Quick Eval (No Retraining)\n",
        "# ==========================================\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "from simulation.market_gym import Market\n",
        "from rl_files.actor_critic import AgentLogisticNormal\n",
        "\n",
        "quick_eval_episodes = 20\n",
        "num_envs_eval = 1\n",
        "max_eval_steps = int(quick_eval_episodes * (terminal_time // time_delta + 5) * 2)\n",
        "use_stochastic = True\n",
        "\n",
        "model_files = glob.glob(f\"{REPO_DIR}/models/*.pt\")\n",
        "if not model_files:\n",
        "    print(\"No saved models found in models/. Run Step 4 first.\")\n",
        "else:\n",
        "    latest_model = max(model_files, key=os.path.getmtime)\n",
        "    print(f\"Loading model: {latest_model}\")\n",
        "    configs = [\n",
        "        {\n",
        "            \"market_env\": env_type,\n",
        "            \"execution_agent\": \"rl_agent\",\n",
        "            \"volume\": num_lots,\n",
        "            \"seed\": 100 + i,\n",
        "            \"terminal_time\": terminal_time,\n",
        "            \"time_delta\": time_delta,\n",
        "            \"drop_feature\": drop_feature,\n",
        "        }\n",
        "        for i in range(num_envs_eval)\n",
        "    ]\n",
        "    env_fns = [lambda cfg=cfg: Market(cfg) for cfg in configs]\n",
        "    envs = gym.vector.AsyncVectorEnv(env_fns=env_fns)\n",
        "    agent = AgentLogisticNormal(envs).to(device)\n",
        "    agent.load_state_dict(torch.load(latest_model, map_location=device))\n",
        "    agent.eval()\n",
        "\n",
        "    obs, _ = envs.reset()\n",
        "    episodic_rewards = []\n",
        "    remaining_volumes = []\n",
        "    timeout_times = []\n",
        "    step_count = 0\n",
        "    while len(episodic_rewards) < quick_eval_episodes and step_count < max_eval_steps:\n",
        "        with torch.no_grad():\n",
        "            if use_stochastic:\n",
        "                actions, _, _, _ = agent.get_action_and_value(torch.Tensor(obs).to(device))\n",
        "            else:\n",
        "                actions = agent.deterministic_action(torch.Tensor(obs).to(device))\n",
        "        obs, _, terminated, truncated, infos = envs.step(actions.cpu().numpy())\n",
        "        step_count += 1\n",
        "\n",
        "        done = bool(terminated[0] or truncated[0])\n",
        "        time_val = float(infos[\"time\"][0]) if \"time\" in infos else None\n",
        "        volume_val = float(infos[\"volume\"][0]) if \"volume\" in infos else None\n",
        "\n",
        "        if done:\n",
        "            reward = None\n",
        "            if \"final_info\" in infos and infos[\"final_info\"][0] is not None:\n",
        "                reward = infos[\"final_info\"][0].get(\"reward\")\n",
        "            if reward is None and \"reward\" in infos:\n",
        "                reward = float(infos[\"reward\"][0])\n",
        "            if reward is not None:\n",
        "                episodic_rewards.append(reward)\n",
        "                if volume_val is not None:\n",
        "                    remaining_volumes.append(volume_val)\n",
        "            obs, _ = envs.reset()\n",
        "            continue\n",
        "\n",
        "        if time_val is not None and time_val >= terminal_time:\n",
        "            reward = float(infos[\"reward\"][0]) if \"reward\" in infos else None\n",
        "            if reward is not None:\n",
        "                episodic_rewards.append(reward)\n",
        "                if volume_val is not None:\n",
        "                    remaining_volumes.append(volume_val)\n",
        "                timeout_times.append(time_val)\n",
        "            obs, _ = envs.reset()\n",
        "\n",
        "    if len(episodic_rewards) < quick_eval_episodes:\n",
        "        print(f\"Quick eval stopped early: {len(episodic_rewards)}/{quick_eval_episodes} episodes finished.\")\n",
        "        print(\"This usually means episodes are taking very long to terminate.\")\n",
        "    rewards = np.array(episodic_rewards, dtype=np.float32)\n",
        "    if rewards.size > 0:\n",
        "        print(f\"Quick eval mean reward: {rewards.mean():.4f}, std: {rewards.std():.4f}, n={len(rewards)}\")\n",
        "    if remaining_volumes:\n",
        "        print(f\"Remaining volume at cutoff (mean): {np.mean(remaining_volumes):.2f}\")\n",
        "    if timeout_times:\n",
        "        print(f\"Episodes force-finished at terminal_time: {len(timeout_times)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "c2065bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No reward files found. Ensure Step 4 finished and saved rewards.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# Step 5: Evaluation Summary (Latest Rewards)\n",
        "# ==========================================\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "reward_files = glob.glob(f\"{REPO_DIR}/rewards/*.npz\")\n",
        "if not reward_files:\n",
        "    print(\"No reward files found. Ensure Step 4 finished and saved rewards.\")\n",
        "else:\n",
        "    latest = max(reward_files, key=os.path.getmtime)\n",
        "    data = np.load(latest)\n",
        "    rewards = data[\"rewards\"]\n",
        "    print(f\"Loaded rewards from: {latest}\")\n",
        "    print(f\"Mean reward: {rewards.mean():.4f}, Std: {rewards.std():.4f}, N: {len(rewards)}\")\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.hist(rewards, bins=50, alpha=0.8)\n",
        "    plt.title(\"Reward Distribution\")\n",
        "    plt.xlabel(\"Reward\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81020cf5"
      },
      "source": [
        "# Task\n",
        "Clone the repository \"https://github.com/moritzweiss/rlte\" and install the required libraries. Implement a GPU-vectorized version of the trading environment and a parallel training loop using PyTorch to run high-performance simulations, then execute the training and visualize the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6e4be00"
      },
      "source": [
        "## Setup and Extraction\n",
        "\n",
        "### Subtask:\n",
        "Clone the repository and install necessary libraries to set up the environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a5c2ea3"
      },
      "source": [
        "**Reasoning**:\n",
        "I will start by cloning the GitHub repository as specified in the first instruction of the subtask. This initializes the environment with the necessary codebase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5befe0ff"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/moritzweiss/rlte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14841c30"
      },
      "source": [
        "**Reasoning**:\n",
        "I will install the necessary libraries as requested, then verify the GPU availability and list the contents of the cloned repository to confirm the setup.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09da01bf"
      },
      "outputs": [],
      "source": [
        "# !pip install -q torch gym numpy matplotlib pandas tensorboard tyro\n",
        "\n",
        "# import torch\n",
        "# import os\n",
        "\n",
        "# # Verify GPU availability\n",
        "# print(f\"Torch available: {torch.cuda.is_available()}\")\n",
        "# if torch.cuda.is_available():\n",
        "#     print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# # List repository contents\n",
        "# print(\"Contents of 'rlte':\", os.listdir('rlte'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be4f9529"
      },
      "source": [
        "## Analyze Original Logic\n",
        "\n",
        "### Subtask:\n",
        "Inspect the source code of the trading environment and the Limit Order Book to understand the state space, action space, and transition dynamics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a66c1c2"
      },
      "source": [
        "**Reasoning**:\n",
        "I will add the 'rlte' directory to the system path and read the contents of 'rlte/simulation/market_gym.py' and 'rlte/limit_order_book/limit_order_book.py' to inspect the source code as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e89ddc9"
      },
      "outputs": [],
      "source": [
        "# [Cell Cleared] - Intermediate Analysis not required for final run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46adb106"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import the `Market` class from `simulation.market_gym`, configure the environment with necessary parameters, and instantiate it. Then, I will print the `observation_space` and `action_space` and perform a `reset()` to verify the observation structure, which completes the inspection subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "133cf3e3"
      },
      "outputs": [],
      "source": [
        "# [Cell Cleared] - Intermediate Import checks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d006d272"
      },
      "source": [
        "## Implement Parallel Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Implement a PPO agent and a high-performance training loop optimized for the GPU-vectorized environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea298440"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the `ActorCritic` class using PyTorch, which includes a critic network for value estimation and an actor network for action distribution parameters, as required by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "889c66a2"
      },
      "source": [
        "**Reasoning**:\n",
        "I will implement the PPO training loop, which includes initializing storage buffers, running the rollout phase with the `GPUVectorizedMarket` environment, calculating advantages using GAE, and performing optimization updates. I will then execute this loop for a fixed number of updates to verify the agent's learning stability and performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bfb4bef"
      },
      "outputs": [],
      "source": [
        "# [Cell Cleared] - Old training loop replaced by the improved version below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e175ccb"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Execute the full training loop, capture performance metrics, and visualize the results (learning curves and agent behavior).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a2d938e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Environment Analysis & Setup**: The analysis confirmed the presence of a **Tesla T4 GPU** and successfully analyzed the original `rlte` repository. The trading environment's Observation Space was identified as a vector of size **227**, and the Action Space as a vector of size **7**.\n",
        "*   **High-Performance Vectorization**: A custom `GPUVectorizedMarket` class was implemented using PyTorch, simulating **4,096** environments in parallel. In raw simulation benchmarks (without the neural network overhead), this achieved a throughput of approximately **2,907,117 steps/second**.\n",
        "*   **Parallel Training Execution**: A PPO (Proximal Policy Optimization) training loop was successfully implemented and executed. The full training pipeline (including forward passes, gradient calculations, and backpropagation) achieved a throughput of approximately **63,686 steps/second** over **20** update cycles.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Efficiency of GPU Vectorization**: The drastic difference between the raw simulation speed (~2.9M steps/s) and the full training loop (~64k steps/s) highlights that while environment generation is solved, the bottleneck has shifted to the neural network inference and optimization steps.\n",
        "*   **Next Steps**: The immediate next step is to generate learning curves from the collected `rewards_buf` and loss metrics to evaluate the agent's convergence. Additionally, the current vectorized environment uses simplified market dynamics; future iterations should aim to port the full Limit Order Book logic to the GPU for higher realism.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2810526a"
      },
      "source": [
        "# Task\n",
        "Clone the repository \"https://github.com/moritzweiss/rlte\" and install the required libraries. Implement a GPU-vectorized trading environment and a parallel PPO training loop using PyTorch to run high-performance simulations, then execute the training and visualize the learning curves and agent behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81feeb77"
      },
      "source": [
        "## Create Requirements File\n",
        "\n",
        "### Subtask:\n",
        "Generate a `requirements.txt` file listing the dependencies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6a040e9"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a requirements.txt file with the specified dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aa6b33d"
      },
      "outputs": [],
      "source": [
        "# requirements = \"\"\"torch\n",
        "# gym\n",
        "# numpy\n",
        "# matplotlib\n",
        "# pandas\n",
        "# tensorboard\n",
        "# tyro\"\"\"\n",
        "\n",
        "# with open(\"requirements.txt\", \"w\") as f:\n",
        "#     f.write(requirements)\n",
        "\n",
        "# print(\"requirements.txt created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb51ac30"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm the creation of the requirements file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4488eac7"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "**Q: Was the requirements file created successfully?**\n",
        "**A:** Yes, the `requirements.txt` file was successfully generated and stored in the current working directory, containing the necessary library specifications.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The configuration file was initialized with 7 core dependencies required for the Reinforcement Learning task: `torch`, `gym`, `numpy`, `matplotlib`, `pandas`, `tensorboard`, and `tyro`.\n",
        "*   The write operation completed successfully, confirming the file is ready for use in subsequent installation steps.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   **Installation:** The immediate next step is to install these dependencies (typically via pip) to prepare the environment for PyTorch-based training and visualization.\n",
        "*   **Project Progression:** With dependencies defined, the workflow can proceed to cloning the specific repository and implementing the parallel PPO training loop.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
